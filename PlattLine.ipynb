{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPI/HCBm+a3HwTngxm9Qow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattPlatt/PLATTLINE_WORKING/blob/main/PlattLine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlWLXWtDII2a",
        "outputId": "a56a77ae-ac9d-42f8-efa1-761a26504710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#connect to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom DataSet Class for Training\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, image_dir, annotation_file, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load JSON annotations\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            self.coco = json.load(f)\n",
        "\n",
        "        # Create a mapping from image ID to file name\n",
        "        self.image_id_to_filename = {img['id']: img['file_name'] for img in self.coco['images']}\n",
        "\n",
        "        # Create a mapping from image ID to annotations\n",
        "        self.image_id_to_annotations = {}\n",
        "        for ann in self.coco['annotations']:\n",
        "            if ann['image_id'] not in self.image_id_to_annotations:\n",
        "                self.image_id_to_annotations[ann['image_id']] = []\n",
        "            self.image_id_to_annotations[ann['image_id']].append(ann)\n",
        "\n",
        "        # List of image IDs\n",
        "        self.image_ids = list(self.image_id_to_filename.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image ID and file name\n",
        "        image_id = self.image_ids[idx]\n",
        "        file_name = self.image_id_to_filename[image_id]\n",
        "        image_path = os.path.join(self.image_dir, file_name)\n",
        "\n",
        "        # Load the image\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB (if grayscale, modify as needed)\n",
        "\n",
        "        # Get annotations for the image\n",
        "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
        "\n",
        "        # Extract bounding boxes and labels\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in annotations:\n",
        "            boxes.append(ann['bbox'])  # COCO format: [x, y, width, height]\n",
        "            labels.append(ann['category_id'])  # Class ID\n",
        "\n",
        "        # Convert to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        # Apply transforms if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "osSrxgnMKgnJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify Dataset Locations for Train and VAl\n",
        "\n",
        "train_image_dir = \"/content/drive/MyDrive/S2gen/data/train/easy/sliced_images\"\n",
        "train_annotation_file = \"/content/drive/MyDrive/S2gen/data/train/easy/sliced_coco_annotations.json\"\n",
        "\n",
        "val_image_dir = \"/content/drive/MyDrive/S2gen/data/val/easy/sliced_images\"\n",
        "val_annotation_file = \"/content/drive/MyDrive/S2gen/data/val/easy/sliced_coco_annotations.json\"\n"
      ],
      "metadata": {
        "id": "yujhwczVKlUF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentor and Data Loader for preprocessing\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Custom collation function to handle varying number of objects per image\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    images = torch.stack(images)  # Stack images into a batch tensor\n",
        "    return images, targets  # Targets remain as a tuple of dictionaries\n",
        "\n",
        "# Define transforms (resize, normalize, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((550, 435)),  # Resize images to the model's input size\n",
        "    transforms.ToTensor(),          # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale image to [-1, 1]\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = COCODataset(image_dir=train_image_dir, annotation_file=train_annotation_file, transform=transform)\n",
        "val_dataset = COCODataset(image_dir=val_image_dir, annotation_file=val_annotation_file, transform=transform)\n",
        "\n",
        "# Create data loaders with custom collation function\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "gabP8e3MKpSQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backbone of Model. This is the NN!\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomBackbone, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # Input channels = 3 (RGB)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))  # Output: 550x435 -> 550x435\n",
        "        x = self.pool(x)           # Output: 550x435 -> 275x217\n",
        "        x = F.relu(self.conv2(x))  # Output: 275x217 -> 275x217\n",
        "        x = self.pool(x)           # Output: 275x217 -> 137x108\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Cihq8KTbI9BS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detection Head Class The RPN is responsible for generating region proposals,\n",
        "# which are potential areas in the image that may contain objects. It uses anchor boxes of\n",
        "#various sizes and aspect ratios, evaluates them to predict Objectness Score and Bounding Box\n",
        "\n",
        "class RegionProposalNetwork(nn.Module):\n",
        "    def __init__(self, in_channels, num_anchors):\n",
        "        super(RegionProposalNetwork, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.cls_logits = nn.Conv2d(128, num_anchors, kernel_size=1, stride=1)  # Objectness scores\n",
        "        self.bbox_pred = nn.Conv2d(128, num_anchors * 4, kernel_size=1, stride=1)  # Box deltas\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv(x))              # Output: 137x108x128\n",
        "        logits = self.cls_logits(x)          # Output: 137x108x(num_anchors)\n",
        "        bbox_deltas = self.bbox_pred(x)      # Output: 137x108x(num_anchors * 4)\n",
        "        return logits, bbox_deltas\n"
      ],
      "metadata": {
        "id": "x5VbrHQPGcIn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Detection Head processes the refined proposals from the RPN. It:\n",
        "\n",
        "# Classifies Each Proposal: Assigns a specific class label (e.g., \"Cables\") or background.\n",
        "# Refines Bounding Boxes: Further adjusts the proposal boxes to fit objects more precisely.\n",
        "\n",
        "class DetectionHead(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(DetectionHead, self).__init__()\n",
        "        self.fc1 = nn.Linear(473472, 256)  # Adjust to match the new flattened size\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.cls_score = nn.Linear(256, num_classes)  # Class scores\n",
        "        self.bbox_pred = nn.Linear(256, num_classes * 4)  # Box deltas\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        scores = self.cls_score(x)\n",
        "        bbox_deltas = self.bbox_pred(x)\n",
        "        return scores, bbox_deltas\n"
      ],
      "metadata": {
        "id": "ZdY2MWOAI5pC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlattLine(nn.Module):\n",
        "    def __init__(self, num_classes, num_anchors):\n",
        "        super(PlattLine, self).__init__()\n",
        "        self.backbone = CustomBackbone()  # Updated backbone\n",
        "        self.rpn = RegionProposalNetwork(in_channels=32, num_anchors=num_anchors)  # Match backbone output channels\n",
        "        self.detection_head = DetectionHead(in_channels=472896, num_classes=num_classes)  # Updated flattened size\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Feature extraction\n",
        "        feature_map = self.backbone(images)  # Output: [8, 32, 137, 108]\n",
        "\n",
        "        # Region proposals\n",
        "        rpn_logits, rpn_bbox_deltas = self.rpn(feature_map)\n",
        "\n",
        "        # Flatten feature map for detection head\n",
        "        flattened_features = torch.flatten(feature_map, start_dim=1)\n",
        "        detection_scores, detection_bbox_deltas = self.detection_head(flattened_features)\n",
        "\n",
        "        return rpn_logits, rpn_bbox_deltas, detection_scores, detection_bbox_deltas\n"
      ],
      "metadata": {
        "id": "WuJdExYpFr1C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN THE MODEL, PRINT THE LOSSES, SAVE THE WEIGHTS, PLOT THE RESULTS\n",
        "\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Initialize model, optimizer, and loss functions\n",
        "num_classes = 1  # One class (\"Cables\") + background\n",
        "num_anchors = 9  # 3 scales x 3 aspect ratios\n",
        "model = PlattLine(num_classes=num_classes, num_anchors=num_anchors)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "classification_loss = torch.nn.CrossEntropyLoss()\n",
        "regression_loss = torch.nn.SmoothL1Loss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "train_losses = []  # To store total loss per epoch\n",
        "rpn_cls_losses = []  # To store RPN classification loss\n",
        "rpn_reg_losses = []  # To store RPN regression loss\n",
        "det_cls_losses = []  # To store detection classification loss\n",
        "det_reg_losses = []  # To store detection regression loss\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_rpn_cls_loss = 0\n",
        "    epoch_rpn_reg_loss = 0\n",
        "    epoch_det_cls_loss = 0\n",
        "    epoch_det_reg_loss = 0\n",
        "\n",
        "    start_time = time.time()  # Track time for the epoch\n",
        "\n",
        "    print(f\"\\n[Epoch {epoch+1}/{num_epochs}] Starting training...\")\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "        images = images.to(device)  # Move images to GPU/CPU directly\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Move annotations to device\n",
        "\n",
        "        # Forward pass\n",
        "        rpn_logits, rpn_bbox_deltas, detection_scores, detection_bbox_deltas = model(images)\n",
        "\n",
        "        print(f\"rpn_logits shape: {rpn_logits.shape}\")\n",
        "        print(f\"rpn_bbox_deltas shape: {rpn_bbox_deltas.shape}\")\n",
        "        print(f\"detection_scores shape: {detection_scores.shape}\")\n",
        "        print(f\"detection_bbox_deltas shape: {detection_bbox_deltas.shape}\")\n",
        "        print(f\"Ground truth labels shape: {torch.cat([t['labels'] for t in targets]).shape}\")\n",
        "        print(f\"Ground truth boxes shape: {torch.cat([t['boxes'] for t in targets]).shape}\")\n",
        "\n",
        "\n",
        "        # Compute losses\n",
        "        rpn_cls_loss = classification_loss(rpn_logits, torch.cat([t['labels'] for t in targets]))\n",
        "        rpn_reg_loss = regression_loss(rpn_bbox_deltas, torch.cat([t['boxes'] for t in targets]))\n",
        "        det_cls_loss = classification_loss(detection_scores, torch.cat([t['labels'] for t in targets]))\n",
        "        det_reg_loss = regression_loss(detection_bbox_deltas, torch.cat([t['boxes'] for t in targets]))\n",
        "\n",
        "        loss = rpn_cls_loss + rpn_reg_loss + det_cls_loss + det_reg_loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update epoch totals\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_rpn_cls_loss += rpn_cls_loss.item()\n",
        "        epoch_rpn_reg_loss += rpn_reg_loss.item()\n",
        "        epoch_det_cls_loss += det_cls_loss.item()\n",
        "        epoch_det_reg_loss += det_reg_loss.item()\n",
        "\n",
        "        # Print per-batch loss (optional, for detailed monitoring)\n",
        "        print(\n",
        "            f\"Batch {batch_idx + 1}/{len(train_loader)} | \"\n",
        "            f\"Total Loss: {loss.item():.4f} | \"\n",
        "            f\"RPN (Cls: {rpn_cls_loss.item():.4f}, Reg: {rpn_reg_loss.item():.4f}) | \"\n",
        "            f\"Det (Cls: {det_cls_loss.item():.4f}, Reg: {det_reg_loss.item():.4f})\"\n",
        "        )\n",
        "\n",
        "    # Track losses for the epoch\n",
        "    train_losses.append(epoch_loss)\n",
        "    rpn_cls_losses.append(epoch_rpn_cls_loss)\n",
        "    rpn_reg_losses.append(epoch_rpn_reg_loss)\n",
        "    det_cls_losses.append(epoch_det_cls_loss)\n",
        "    det_reg_losses.append(epoch_det_reg_loss)\n",
        "\n",
        "    end_time = time.time()  # End epoch timer\n",
        "\n",
        "    # Print concise epoch summary\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1}/{num_epochs} | \"\n",
        "        f\"Total Loss: {epoch_loss:.4f} | \"\n",
        "        f\"RPN (Cls: {epoch_rpn_cls_loss:.4f}, Reg: {epoch_rpn_reg_loss:.4f}) | \"\n",
        "        f\"Det (Cls: {epoch_det_cls_loss:.4f}, Reg: {epoch_det_reg_loss:.4f}) | \"\n",
        "        f\"Time: {end_time - start_time:.2f}s\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Save the model weights after all epochs are completed\n",
        "torch.save(model.state_dict(), \"PlattLine_final.pth\")\n",
        "print(\"\\nTraining complete. Model weights saved to 'PlattLine_final.pth'\")\n",
        "\n",
        "# Plot individual loss components\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Total Loss\")\n",
        "plt.plot(range(1, num_epochs + 1), rpn_cls_losses, label=\"RPN Classification Loss\")\n",
        "plt.plot(range(1, num_epochs + 1), rpn_reg_losses, label=\"RPN Regression Loss\")\n",
        "plt.plot(range(1, num_epochs + 1), det_cls_losses, label=\"Detection Classification Loss\")\n",
        "plt.plot(range(1, num_epochs + 1), det_reg_losses, label=\"Detection Regression Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss Components Over Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YFG8j2cRJgeU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "975d7ffe-6a15-4209-ed37-071c7263c082"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Epoch 1/10] Starting training...\n",
            "rpn_logits shape: torch.Size([8, 9, 137, 108])\n",
            "rpn_bbox_deltas shape: torch.Size([8, 36, 137, 108])\n",
            "detection_scores shape: torch.Size([8, 1])\n",
            "detection_bbox_deltas shape: torch.Size([8, 4])\n",
            "Ground truth labels shape: torch.Size([87])\n",
            "Ground truth boxes shape: torch.Size([87, 4])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (8) to match target batch_size (87).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ee39f4b12cb0>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Compute losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mrpn_cls_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpn_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mrpn_reg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregression_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpn_bbox_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'boxes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mdet_cls_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (8) to match target batch_size (87)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "\n",
        "def plot_predictions(image, ground_truth, predictions):\n",
        "    \"\"\"\n",
        "    Visualize ground truth and predicted bounding boxes on the image.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())  # Convert tensor to NumPy for plotting\n",
        "\n",
        "    # Plot ground truth\n",
        "    for bbox in ground_truth[\"boxes\"]:\n",
        "        x, y, w, h = bbox\n",
        "        rect = plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='g', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x, y - 5, \"GT: Cables\", color='green', fontsize=12)\n",
        "\n",
        "    # Plot predictions\n",
        "    for bbox, score in zip(predictions[\"boxes\"], predictions[\"scores\"]):\n",
        "        x, y, w, h = bbox\n",
        "        rect = plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x, y + h + 5, f\"Pred: {score:.2f}\", color='red', fontsize=12)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "LGj5Ft55PEFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on Validation data.\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for images, targets in val_loader:\n",
        "    images = [img.to(device) for img in images]\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        rpn_logits, rpn_bbox_deltas, detection_scores, detection_bbox_deltas = model(images)\n",
        "\n",
        "    # Convert outputs to usable predictions\n",
        "    predictions = {\n",
        "        \"boxes\": detection_bbox_deltas[0].cpu().numpy(),  # Placeholder for processed boxes\n",
        "        \"scores\": torch.softmax(detection_scores[0], dim=1)[:, 1].cpu().numpy(),  # Confidence for \"Cables\"\n",
        "    }\n",
        "\n",
        "    # Visualize the first image in the batch\n",
        "    plot_predictions(images[0].cpu(), targets[0], predictions)\n",
        "    break  # Only visualize one batch for now\n"
      ],
      "metadata": {
        "id": "DnRWmXv_PJlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot precision and recall\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Assume ground_truths and predictions are available in the following format:\n",
        "# ground_truths = [{'boxes': [[x1, y1, w1, h1], ...], 'labels': [class_id, ...]}, ...]\n",
        "# predictions = [{'boxes': [[x1, y1, w1, h1], ...], 'labels': [class_id, ...], 'scores': [conf, ...]}, ...]\n",
        "\n",
        "# Flatten ground truth and predicted labels\n",
        "all_gt_labels = []\n",
        "all_pred_labels = []\n",
        "\n",
        "for gt, pred in zip(ground_truths, predictions):\n",
        "    all_gt_labels.extend(gt[\"labels\"])\n",
        "    all_pred_labels.extend(pred[\"labels\"])\n",
        "\n",
        "# Compute precision and recall\n",
        "precision = precision_score(all_gt_labels, all_pred_labels, average=\"macro\")\n",
        "recall = recall_score(all_gt_labels, all_pred_labels, average=\"macro\")\n",
        "\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "brE4AgUFPZOo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}